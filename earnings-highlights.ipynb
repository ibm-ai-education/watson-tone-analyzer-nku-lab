{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze earnings call transcripts with Watson Tone Analyzer and Natural Language Understanding\n",
    "\n",
    "This lab uses the Tone Analyzer service to extract the most positive sentences  from earnings call transcripts and then runs those remarks through the Natural Language Understanding  service to extract the most relevant keywords and Semantic Roles  from those  sentences .\n",
    "\n",
    "After creating instances of Tone Analyzer and Natural Language Understanding, you'll test drive the scenario with a Python application in a Jupyter notebook using Watson Studio.\n",
    "\n",
    "## Setup\n",
    "\n",
    "1. Download the zip file with the earrnings call transcripts from [here](https://github.com/ibm-ai-education/watson-tone-analyzer-nlu-lab/raw/master/test_data/1Q2018.zip) to your local system. The name of the file is **1Q2018.zip**\n",
    "2. Unzip the file to a local folder.\n",
    "3. Click on the data icon at the top right of the notebook window and then selectand upload the 4 earnings transcript files\n",
    "\n",
    "![Data icon](https://github.com/ibm-ai-education/watson-tone-analyzer-nlu-lab/raw/master/images/ss11.png)\n",
    "\n",
    "4. Once the files are uploaded, place your cursor in the code cell below and select **Insert to code->Insert Credentials** under any of the  4 files you just  uploaded. This will insert the credentails that will allow you to access the uploaded files from the notebook code.\n",
    "\n",
    "<img src=\"https://github.com/ibm-ai-education/watson-tone-analyzer-nlu-lab/raw/master/images/ss12.png\"  alt=\"Add creds\"  width=\"335\" height=\"497\" />\n",
    "\n",
    "5. Run each cell in the notebook after reading the description of what is being done with each cell\n",
    "\n",
    "### Cloud Object Storage Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With your cursor in this cell, insert the code with the Cloud Object Storage credentials as instructed in step 4) \n",
    "# of the setup instructions above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate client to access Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_botocore.client import Config\n",
    "import ibm_boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this code assumes inserted credential dictionary  in the first code block above is\n",
    "# called credentials_1. If yours is called differently adjust the following line of code \n",
    "# accordingly\n",
    "credentials = credentials_1 \n",
    "\n",
    "cos = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
    "    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
    "    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=credentials['ENDPOINT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data files from IBM Cloud Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create dir for data files \n",
    "test_data_dir = 'test_data'\n",
    "if not os.path.isdir(test_data_dir):\n",
    "    os.mkdir(test_data_dir)\n",
    "    \n",
    "cos.download_file(Bucket=credentials['BUCKET'],Key='Citigroup1Q2018EarningsCall.txt',Filename='test_data/Citigroup1Q2018EarningsCall.txt')\n",
    "cos.download_file(Bucket=credentials['BUCKET'],Key='JPMC1Q2018EarningsCall.txt',Filename='test_data/JPMC1Q2018EarningsCall.txt')\n",
    "cos.download_file(Bucket=credentials['BUCKET'],Key='MorganStanley1Q2018EarningsCall.txt',Filename='test_data/MorganStanley1Q2018EarningsCall.txt')\n",
    "cos.download_file(Bucket=credentials['BUCKET'],Key='WellsFargo1Q2018EarningsCall.txt',Filename='test_data/WellsFargo1Q2018EarningsCall.txt')\n",
    "\n",
    "!ls test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install latest Watson API client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ibm-watson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Watson service clients\n",
    "\n",
    "Insert the values of the API keys for Watson Tone Analyzer and Watson Natural Language Understanding that you saved earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson import ToneAnalyzerV3\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, KeywordsOptions, SemanticRolesOptions\n",
    "\n",
    "# Insert your API keys here\n",
    "tone_analyzer_apikey = \"***********\"\n",
    "nlu_apikey = \"**************\"\n",
    "       \n",
    "\n",
    "# Create service clients\n",
    "tone_analyzer = ToneAnalyzerV3(iam_apikey= tone_analyzer_apikey, version='2017-09-21')\n",
    "\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(version='2018-03-16', iam_apikey=nlu_apikey)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through data files selecting top 5 most positive statements and then doing an  NLU analysis of those statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "from operator import itemgetter\n",
    "\n",
    "# Loop through all call transcript files\n",
    "test_files = glob.glob(test_data_dir + '/**/*.txt', recursive=True)\n",
    "print('Analyzing  %d earnings call transcripts ...' % (len(test_files)))\n",
    "for filename in  test_files:\n",
    "   print(\"Analyzing transcript file name \" + filename)\n",
    "\n",
    "   with open(filename, 'r') as transcript:\n",
    "\n",
    "      tone = tone_analyzer.tone(tone_input=transcript.read(), content_type=\"text/plain\").get_result()\n",
    "      \n",
    "      # Get joy and sort by descending score\n",
    "      sentences_with_joy = []\n",
    "      for each_sentence in tone['sentences_tone']:\n",
    "         for each_tone in each_sentence['tones']:\n",
    "            if each_tone['tone_id'] == 'joy':\n",
    "               sentences_with_joy.append({'sentence_id': each_sentence['sentence_id'], 'text': each_sentence['text'], 'score': each_tone['score']})\n",
    "               break\n",
    "\n",
    "      sentences_with_joy = sorted(sentences_with_joy, key=itemgetter('score'), reverse=True)\n",
    "      # Only top 5 are being selected\n",
    "      if len(sentences_with_joy) > 5:\n",
    "         sentences_with_joy = sentences_with_joy[:5]\n",
    "\n",
    "\n",
    "      index = 1\n",
    "      print('\\nMost positive statements from earnings call:\\n')\n",
    "\n",
    "      # Go through top positive sentences and use NLU to get keywords and\n",
    "      # Semantic Roles\n",
    "      for sentence in sentences_with_joy:\n",
    "         print(str(index) + ') ' + sentence['text'])\n",
    "         nlu_analysis = natural_language_understanding.analyze(text = sentence['text'], features=Features(keywords=KeywordsOptions(), semantic_roles=SemanticRolesOptions(keywords=True))).get_result()\n",
    "         first_keyword = True\n",
    "         for each_item in nlu_analysis['keywords']:\n",
    "            if first_keyword:\n",
    "                print('')\n",
    "                print('NLU Analysis:')\n",
    "                print('keywords: ' + each_item['text'], end='')\n",
    "                first_keyword = False\n",
    "            else:\n",
    "                print(', ' + each_item['text'], end='')\n",
    "         print('')\n",
    "         first_semantic_role = True\n",
    "         for each_item in nlu_analysis['semantic_roles']:\n",
    "            if first_semantic_role:\n",
    "               print('semantic_roles:')\n",
    "               first_semantic_role = False\n",
    "            subject_dict = each_item.get('subject')\n",
    "            if subject_dict is None:\n",
    "               print('subject: N/A ', end='')\n",
    "            else:\n",
    "               print('subject: ' + subject_dict['text'], end=' ')\n",
    "\n",
    "            action_dict = each_item.get('action')\n",
    "            if action_dict is None:\n",
    "               print('action: N/A ', end='')\n",
    "            else:\n",
    "               print('action: ' + action_dict['text'], end=' ')\n",
    "\n",
    "            object_dict = each_item.get('object')\n",
    "            if object_dict is None:\n",
    "               print('object: N/A', end='')\n",
    "            else:\n",
    "               print('object: ' + object_dict['text'], end='')\n",
    "            print()\n",
    "\n",
    "         index = index + 1\n",
    "         print('\\n')\n",
    "\n",
    "print('Processing complete.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
